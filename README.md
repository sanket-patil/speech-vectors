# Speech Vectors

#### Towards learning "good" representations for speech data.

[WIP]

#### Representations

Properties or features of a domain are transformed to a mathematical form -- essentially a set of numbers, or "feature vectors". 
One of the main factors affecting the performance of machine learning algorithms is 
input data representation. 
Good representations are useful as inputs to a predictor (classifier or a regressor). 
They also capture the underlying patterns very well.
Since representations are crucial, feature engineering has been central 
to machine learning. However, it would be great if we can learn 
representations automatically.

#### Importance of representations

Good reprsentations essentially have the effect of "disentanglement".
That is to say they highlight the variances in underlying data clearly.

#### Learning Representations for Speech
There have been a variety of features proposed for speech based on
power spectrum analysis; tonal analysis; pitch analysis; and so on.

#### Possibility of "speech embeddings"?
Word embeddings contributed hugely to the current success of NLP. 
We start with very basic features, which are words or characters, and
allow a neural network to learn more complex and rich representations.

Can we do this for speech without closely modeling the human speech and 
hearing system? Can we arrive at pretrained speech embeddings that can be 
used across multiple tasks ("transfer learning").

#### Multi-modal embeddings
Can we augment audio features with text features for 

#### TO DO
- Acoustic Language Models
- Auto Encoders for learning embeddings
- 



